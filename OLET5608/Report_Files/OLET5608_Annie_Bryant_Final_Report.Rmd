---
title: "Exploring cognitive performance with linear modeling"
authors:
- name: Annie G. Bryant
  affiliation: OLET5608 May 2022
  email: abry4213@uni.sydney.edu.au
abstract: |
  Placeholder.
output:
  pdf_document: default
  html_document: default
bibliography: references.bib
keywords: placeholder
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
```

```{r, echo = FALSE, message = FALSE, warning = FALSE}
# Load packages
library(tidyverse)
library(ggfortify)
library(lmtest)
library(pinp)
library(cowplot)
library(corrplot)
library(fastDummies)
library(patchwork)
library(knitr)
library(kableExtra)
library(car)
library(gridExtra)
library(lmtest)
library(sandwich)
theme_set(theme_cowplot())
plot_path <- "../Report_Images/"
```

# 1. Introduction

Alzheimer's disease (AD) is a neurodegenerative condition that is the primary cause of dementia globally [1]. One of the primary neuropathological hallmarks of AD is the aggregation of hyperphosphorylated tau protein (p-Tau) into neurofibrillary tangles inside neurons [2]. While this occurs within the brain in AD, such pathological changes can also be detected in the cerebrospinal fluid (CSF) that flows between the brain and spinal cord. Several landmark studies over the past decade have shown that elevated levels of p-Tau protein in the CSF correlate with the progression of tau pathology in the brain, and can even predict onset of cognitive decline years in advance. Given the relevance of this exciting biomarker, I sought to leverage open-access AD biomarker data to better understand the cognitive and biological factors that relate to increased CSF p-Tau via linear modelling.

# 2. Data set
 
Cross-sectional cognitive, neuroanatomical, and demographic data were acquired from the Alzheimer’s Disease Neuroimaging Initiative (ADNI) [3], which is a multi-centre neuroimaging consortium that provides open-access AD biomarker data. After preliminary data cleaning, I retained a dataset with N=701 observations (i.e. participants), N=1 outcome variable (CSF p-Tau levels), and N=19 predictor variables. These predictor variables can be generally organized into six categories: Cognitive (ADAS-114, ADAS-135, MMSE6, CDR-SB7), Demographic (age, gender, education, race, marital status), genetic (APOE gene E4 allele status), neuroimaging (fluorodeoxyglucose positron emission tomography; FDG), neuroanatomical (volume of ventricles, hippocampus, whole brain, entorhinal, fusiform, middle temporal cortex, total intracranial region), and peripheral biomarker (CSF amyloid-beta protein levels). The first four values and a description of each of these features is provided in Table 1 below.

Of note, I manually encoded PTGENDER, PTRACCAT, PTMARRY, and APOE4 as factors since they are classes rather than continuously distributed numerical variables. I also discretised PTEDUCAT into a new variable (PTHIGHERED) two groups: (1) 16 or fewer years and (2) greater than 16 years, since this is not a truly continuous variable.

```{r, echo = FALSE}
# We can start by reading in the data, provided in a CSV format. Here's the first four values and description of each feature in the dataset:
data_file <- "../Project_Dataset/ADNIMERGE.csv"
description_file <- read.csv("../Project_Dataset/Variable_Descriptions.csv")
ADNI_data <- read.csv(data_file, na.strings = c("NA", "")) %>%
  
  # Select a subset of columns of interest
  dplyr::select(RID, EXAMDATE, AGE, PTGENDER, PTEDUCAT, PTRACCAT, PTMARRY, APOE4, FDG, ABETA, PTAU, MMSE, CDRSB, ADAS11, ADAS13, Ventricles, Hippocampus, WholeBrain, Entorhinal, Fusiform, MidTemp, ICV) %>%
  
  # Convert exam date to date format
  mutate(EXAMDATE = as.Date(EXAMDATE)) %>%
  
  # Convert numeric variables to be treated as numeric
  mutate_at(c("ABETA", "PTAU"),
            function(x) as.numeric(x)) %>%
  group_by(RID) %>%
  
  # Drop na values for these columns and only keep
  # participants where marital status is known
  filter(!is.na(MMSE), !is.na(PTAU), !is.na(Entorhinal),
         !is.na(Hippocampus), !is.na(ABETA), 
         PTMARRY != "Unknown", PTRACCAT != "Unknown") %>%
  
  # Pick the first visit date available per subject
  filter(EXAMDATE == min(EXAMDATE, na.rm=T)) %>%
  dplyr::select(-EXAMDATE) %>%
  drop_na() %>%
  ungroup() %>%
  # Set APOE4 to boolean and subject ID to character
  mutate(APOE4 = ifelse(APOE4>0, TRUE, FALSE),
         RID = as.character(RID))

head_data <- head(ADNI_data, 4) %>%
  t()

# Table 1
as.data.frame(head_data) %>%
  unite("First 4 Values", V1:V4, sep=", ") %>%
  rownames_to_column(var="Variable") %>%
  left_join(., description_file) %>%
  kable(.) %>%
  kable_styling(full_width = F) %>%
  column_spec(1, width = "6em") %>%
  column_spec(2, width = "10em")
```


```{r}
# I manually encoded `PTGENDER`, `PTRACCAT`, `PTMARRY`, and `APOE4` as factors, since they are classes rather than continuously distributed numerical variables. I also discretized PTEDUCAT into a new variable (PTHIGHERED) two groups: (1) 16 or fewer years and (2) greater than 16 years, since this is not a truly continuous variable.
# Manually encode factors
ADNI_data <- ADNI_data %>%
  mutate(PTGENDER = factor(PTGENDER, levels=unique(PTGENDER)),
         PTRACCAT = factor(PTRACCAT, levels=unique(PTRACCAT)),
         PTMARRY = factor(PTMARRY, levels=unique(PTMARRY)),
         APOE4 = factor(APOE4, levels = c(FALSE, TRUE)))

# Discretize PTHIGHERED
ADNI_data$PTHIGHERED <- ifelse(ADNI_data$PTEDUCAT > 16, TRUE, FALSE)
ADNI_data <- ADNI_data %>% 
  dplyr::select(-PTEDUCAT) %>%
  mutate(PTHIGHERED = factor(PTHIGHERED, levels = c(FALSE, TRUE)))
```


# 3. Analysis

## 3.1 Exploratory data analysis and visualization

Before beginning any modelling, I visualized the univariate distribution for the predictor and outcome variables in this dataset (Figure 1). Most of the quantitative (continuously-distributed) predictor terms are somewhat normally distributed, with the exception of ADAS11, ADAS13, CDRSB, and Ventricles which exhibit positive skews; additionally, the outcome feature (PTAU) also exhibits a positive skew. 

```{r, echo = FALSE, fig.width=6, fig.height=5}
# Figure 1

# Function to emulate the ggplot default color palette for a given number of features
gg_color_hue <- function(n) {
  hues = seq(15, 375, length = n + 1)
  hcl(h = hues, l = 65, c = 100)[1:n]
}

# Function to create either a histogram or bar chart for each feature
# Depending on whether it's numerical or categorical, respectively
plot_func <- function(df, plot_list) {
  num_features <- length(colnames(df))
  plot_palette <- gg_color_hue(num_features)
  for (i in 1:length(colnames(df))) {
    feature <- sort(colnames(df))[i]
    if (is.numeric(df %>% pull(feature))) {
      feature_p <- df %>%
        dplyr::select(all_of(feature)) %>%
        dplyr::mutate(row_id = dplyr::row_number()) %>%
        pivot_longer(cols=c(-row_id),
                     names_to = "Variable",
                     values_to = "Raw_Value") %>%
        ggplot(data=., mapping=aes(x=Raw_Value)) +
        geom_histogram(fill = plot_palette[i]) 
    } else {
      feature_p <- df %>%
        dplyr::select(all_of(feature)) %>%
        dplyr::mutate(row_id = dplyr::row_number()) %>%
        pivot_longer(cols=c(-row_id),
                     names_to = "Variable",
                     values_to = "Value") %>%
        group_by(Variable, Value) %>%
        count() %>%
        ggplot(data=., mapping=aes(x = Value, y = n)) +
        geom_bar(stat = "identity", fill = plot_palette[i])
    }
    feature_p <- feature_p  +
        facet_wrap(Variable ~ ., scales="free") +
        theme(axis.text.x = element_text(angle=90, hjust=1, vjust=0.4, size=7),
              axis.text.y = element_text(size=7),
              plot.title = element_text(hjust=0.5, size=9),
              axis.title = element_blank(),
              strip.text = element_text(size=8),
              legend.position = "none")
    plot_list <- rlist::list.append(plot_list, feature_p)
  }
  
  return(plot_list)
}

# Generate list of plots, one per ADNI dataset feature (other than RID)
plot_list <- plot_func(ADNI_data %>% dplyr::select(-RID),
                       list())

# Draw the plots
patchwork::wrap_plots(plot_list, ncol=5) + plot_annotation(
  title = 'Distribution of Features in ADNI Dataset'
) & theme(plot.margin = unit(c(0.05, 0.05, 0.05, 0.05), "cm"),
          plot.title = element_text(size=12))
grid::grid.draw(grid::textGrob("Number of Subjects", x = 0.01, rot = 90))
grid::grid.draw(grid::textGrob("Feature Value", y = 0.01))

# Save to a PNG
ggsave(paste0(plot_path, "Figure1_Distribution_of_Numerical_Features_in_ADNI_Dataset.png"),
       width=6, height=5, units="in", dpi=300)
```



```{r, message=F, warning=F}
# Convert factors to dummy binary variables
ADNI_data_dummy <- dummy_cols(ADNI_data, 
           select_columns = c("PTGENDER", "PTRACCAT", "PTMARRY", "PTHIGHERED", "APOE4"),
           remove_first_dummy = TRUE,
           remove_selected_columns = TRUE)

# Function to calculate z-score per feature
z_score <- function(variable_values) {
  z_scores <- (variable_values - mean(variable_values, na.rm=T))/sd(variable_values, na.rm=T)
  return(z_scores)
}

# Extract qualitative features + outcome variable
ADNI_qual <- ADNI_data %>%
  dplyr::select(RID, PTAU, PTGENDER, PTRACCAT, PTMARRY, APOE4, PTHIGHERED)

# Apply z-score function to quantitative features
ADNI_z_quant <- ADNI_data %>%
  dplyr::select(-PTAU) %>%
  dplyr::select(AGE, FDG:ICV) %>%
  dplyr::select_if(is.numeric) %>%
  # Create arbitrary ID to enable pivoting
  dplyr::mutate(row_id = dplyr::row_number()) %>%
  pivot_longer(cols=c(-row_id),
               names_to = "Variable",
               values_to = "Raw_Value") %>%
  # Group by variable
  group_by(Variable) %>%
  mutate(Z_Value = z_score(Raw_Value)) %>%
  pivot_wider(id_cols = row_id, names_from=Variable, values_from=Z_Value) %>%
  dplyr::select(-row_id) %>%
  cbind(., RID=ADNI_data$RID)

# Merge z-scored data back with qualitative features
ADNI_data_z <- left_join(ADNI_z_quant, ADNI_qual)
```

## 3.2 Model fitting and diagnostics

There are four core assumptions of a linear regression model [@marill2004advanced]:  

1. A linear relationship between the predictor and outcome variable(s)
2. Normal distribution of residuals
3. Constant variation across residuals
4. Independent observations 


### 3.2.1 Linear relationship

Given the vast differences in scale between e.g. FDG and WholeBrain, I first applied z-score normalization before constructing any models, in which each feature is mean-centred and standardized. This technique puts each quantitative variable in the same scale, thus enabling direct comparison of coefficients derived from subsequent linear models. Once z-scored, the linear relationship between each quantitative predictor and CSF PTAU can be visually assessed with scatterplots (Figure 2). Of note, the y-axis in Figure 2 shows log-transformed CSF PTAU levels, which is further discussed in Section 3.2.2. There is a visually positive linear association between the outcome (PTAU levels in CSF) and ADAS11, ADAS13, AGE, and a visually negative linear association with ABETA, Entorhinal, FDG, Fusiform, Hippocampus, ICV, MidTemp, MMSE, and Ventricles, WholeBrain.

I constructed a basic ordinary least squares (OLS) linear regression model using all z-scored features in the dataset with PTAU as the outcome variable as a baseline; this model will be referred to as Model 1. To assess potential multicollinearity, I plotted the Pearson correlation between all predictor terms (Figure 3). Four features stand out as having particularly high correlation magnitudes: ADAS11, ADAS13, WholeBrain, and ICV. This was confirmed by calculating the variance inflation factor (VIF) across all predictor terms; all four of these terms had VIF values greater than 5, which is generally considered to be a threshold above which multicollinearity will be a problem [9]. Since ADAS13 encompasses all of the ADAS11 test components with the addition of two new components, I dropped ADAS11 going forward. Additionally, WholeBrain captures just the brain tissue volume whereas ICV also contains the volume of CSF in the brain, so I opted to keep WholeBrain and drop ICV going forward. This OLS model in which ADAS11 and ICV are omitted will be referred to as Model 2.


```{r, echo = FALSE, fig.width=8, fig.height=4.5}
ADNI_z_quant %>%
  cbind(., dplyr::select(ADNI_data, c("PTAU"))) %>%
  pivot_longer(cols = c(-RID, -PTAU),
               names_to = "Predictor_Variable",
               values_to = "Value") %>%
  ggplot(data=., mapping=aes(x=Value, y=log(PTAU))) +
  geom_point(aes(color = Predictor_Variable), alpha = 0.5) +
  facet_wrap(Predictor_Variable ~ ., scales="free", ncol=5) +
  theme(legend.position = "none") +
  ylab("log-PTAU in CSF") +
  xlab("Predictor Value (Z-Scored)") +
  geom_smooth(method = "lm", color = "black")

ggsave(paste0(plot_path, "Figure2_Linear_Relationship_to_Outcome_Variable.png"),
       width=8, height=4.5, units="in", dpi=300)
```


```{r, echo = FALSE, fig.width=6, fig.height=5}
# Construct OLS
model1 <- lm(PTAU ~ .,
             data = dplyr::select(ADNI_data_z,
                                  c(-RID)))

# Correlation plot among predictor variables
corr_mat <- ADNI_data_dummy %>%
  dplyr::select(-RID, -PTAU) %>%
  cor(., method = "pearson", use = "complete.obs")

ggplot(reshape2::melt(corr_mat), aes(Var1, Var2, fill=value)) +
  geom_tile(height=0.8, width=0.8) +
  scale_fill_gradient2(low="blue", mid="white", high="red") +
  theme_minimal() +
  coord_equal() +
  ggtitle("Pearson Correlation among Predictor Variables") +
  labs(x = "", y = "",fill="Pearson's\nR") +
  theme(axis.text.x=element_text(size=7, angle=45, vjust=1, hjust=1, 
                                 margin=margin(-3,0,0,0)),
        axis.text.y=element_text(size=8, margin=margin(0,-3,0,0)),
        plot.title = element_text(hjust=0.5, face="bold", size=14),
        panel.grid.major=element_blank()) 

ggsave(paste0(plot_path, "Figure3_ADNI_Feature_Corrplot.png"),
       width=6, height=5, units="in", dpi=300)
```

```{r}
cat("\nVariance Inflation Factor (VIF) values above 5:\n")
vif(model1)[,1][vif(model1)[,1] > 5]
```


```{r}
# Construct OLS without multicollinearity
model2 <- lm(PTAU ~ .,
             data = dplyr::select(ADNI_data_z,
                                  c(-RID, -ADAS11, -ICV)))
```



### 3.2.2 Normal distribution of residuals

The second assumption of a linear model is that the residuals are normally distributed. This assumption can be assessed using a Q-Q plot, which shows the quartiles of the standardized residuals (y axis) versus the quartiles of a normal distribution (x axis). Figure 4 shows the Q-Q plot and histogram of residual values for Model 2, in which the residuals are clearly positively skewed. To mitigate this issue, I tried transforming the outcome variable (PTAU) with a Box-Cox power transformation (Figure 5) [10]. In this type of visualization, the centre dashed line around 0 represents 𝜆, and the outer two vertical lines are the 95% confidence interval for 𝜆. 

While the 95% confidence interval does not cover any 𝜆 value associated with a typical power transformation, we might still try using 𝜆 = 0 for a log transformation of the outcome variable (PTAU). I visually inspected the distribution after log-transforming PTAU versus square root-transforming or inverse-transforming it (Figure 6), which confirms that the log transform yields the closest to a normal distribution of PTAU. After adding log-transformed PTAU to Model 2 (which is hereafter referred to as Model 3), I visualized the Q-Q plot and residual histogram again (Figure 7). The residuals do more closely follow a normal distribution now, and given the large sample size (N=701 observations), it is safe to assume that this distribution is sufficiently normal based on the central limit theorem.



```{r, echo = FALSE, fig.keep = TRUE, fig.width = 7, fig.height = 3.5}
qq <- autoplot(model2, which = 2)
residual_histogram <- data.frame(Residual = model2$residuals) %>%
  ggplot(data=., mapping=aes(x=Residual)) +
  geom_histogram() +
  ggtitle("Distribution of OLS\nModel Residuals")
png(paste0(plot_path, "Figure4_Normal_Distribution_of_OLS_Residuals.png"),
    width=7, height=3.5, units="in", res=300)
qq + residual_histogram
dev.off()
```

```{r, fig.width=5, fig.height=3.5}
# Figure 5: Box-Cox plot
library(MASS)
png(paste0(plot_path, "Figure5_OLS_BoxCox_Plot.png"),
    width=5, height=3.5, units="in", res=300)
boxcox(model2, plotit=T)
dev.off()
```

```{r, fig.width=7, fig.height=3}
# Figure 6 possible PTAU transforms
ADNI_data %>%
  dplyr::select(PTAU) %>%
  dplyr::rename("Original_Value" = "PTAU") %>%
  mutate(Log_Value = log(Original_Value),
         Sqrt_Value = sqrt(Original_Value),
         Inv_Value = 1/Original_Value) %>%
  pivot_longer(cols=c(Log_Value, Sqrt_Value, Original_Value, Inv_Value)) %>%
  mutate(name = factor(name, levels = c("Original_Value",
                                        "Log_Value",
                                        "Sqrt_Value",
                                        "Inv_Value"))) %>%
  ggplot(data=., mapping=aes(x=value)) +
  ggtitle("Distribution of PTAU with various power transforms") +
  geom_histogram(aes(fill=name)) +
  facet_grid(. ~ name, scales="free") +
  theme(plot.title = element_text(hjust=0.5),
        legend.position = "none",
        axis.text.x = element_text(angle=90, vjust=0.4, hjust=1)) +
  xlab("Value") +
  ylab("Number of Subjects")
ggsave(paste0(plot_path, "Figure6_PTAU_Transformations.png"),
    width=7, height=3, units="in", dpi=300)
```


```{r}
# Model 3
model3 <- lm(log(PTAU) ~ ., 
             data = dplyr::select(ADNI_data_z,
                                  c(-RID, -ADAS11, -ICV)))
qq <- autoplot(model3, which = 2)
residual_histogram <- data.frame(Residual = model3$residuals) %>%
  ggplot(data=., mapping=aes(x=Residual)) +
  geom_histogram() +
  ggtitle("Distribution of Residuals\nfor Model 3")
png(paste0(plot_path, "Figure7_Normal_Distribution_of_OLS_Residuals_logPTAU_Model3.png"),
    width=7, height=3.5, units="in", res=300)
qq + residual_histogram
dev.off()
```

### 3.2.3 Constant variation across residuals

The third assumption of a linear model is that the residuals exhibit a consistent variance across all fitted values in the model -- in other words, that there is no discernible bias of the variance across residuals. We can evaluate this assumption by plotting either the raw residuals or the square root of the residuals versus the fitted values using the ggfortify::autoplot function, comparing Model 2 with Model 3 (Figure 8). For a linear model with constant variance across the fitted outcome variable values, we would expect to see a flat blue line in all four of these plots and no clear trend in residuals based on fitted values. However, the upper two plots (Figure 8A-B) show a funneling effect in which the residuals fan out as the fitted values increase, indicating there is heteroscedasticity present in Model 2. I applied the Breusch-Pagan test to Model 2, which returned p<0.0001, supporting the conclusion that there is heteroscedasticity present in this model. The residual plots look consistent for Model 3, but since the Breusch-Pagan test for Model 3 returned p<0.01, I compared this model with a weighted least squares (WLS) regression model Model 4.

```{r, echo = FALSE, fig.width=7, fig.height=6}
# Figure 8 residual variance for models 2 and 3
png(paste0(plot_path, "Figure8_Constant_Residual_Variance_autoplot_models_2_3.png"),
    width=7, height=6, units="in", res=300)
model2_autoplot <- autoplot(model2, which = c(1,3))
model3_autoplot <- autoplot(model3, which = c(1,3))

grid.arrange(model2_autoplot@plots[[1]], model2_autoplot@plots[[2]],
             model3_autoplot@plots[[1]], model3_autoplot@plots[[2]], nrow = 2)
dev.off()
```

```{r}
# Run Breusch-Pagan tests

#perform Breusch-Pagan test for model 2
bptest(model2)

#perform Breusch-Pagan test for model 3
bptest(model3)
```


While heteroscedasticity does not appear to be an issue for Model 3 (Figure 8C-D), the Breusch-Pagan test still returned p<0.01. As heteroscedasticity is an issue for linear models – particularly in estimating standard errors – I opted to calculate robust standard errors using a heteroscedasticity-robust estimator [11–13]. With this method, standard errors are calculated with the sandwich estimator of variance and are valid even in the presence of heteroscedasticity -- and are often larger than conventionally estimated standard errors.

```{r}
# Model 4 = robust standard errors for model 3
model4 <- coeftest(model3, vcov = vcovHC(model3, "HC3"), save=T)
```

### 3.2.4 Independence of observations

The final assumption of a linear model is that the observations are independent. The ADNI dataset I used in this project contains clinical and cognitive data for N=704 distinct and independent individuals. While it is possible that some individuals in this dataset are biologically related, and therefore may not have fully independent measurements, such information is not available. As such, I will assume that the observations in this dataset are all independent.

# 4. Results

## 4.1 Model selection

```{r}
model5 <- stepAIC(model3, direction = "both", 
                      trace = FALSE)
summary(model5)
```

## 4.2 Model summary

```{r}
# Stepwise regression model
model5_df <- data.frame(model5_coefs = round(model5$coefficients, 3),
                        model5_SE = round(summary(model5)$coefficients[,2], 3),
                        model5_pvals = summary(model5)$coefficients[,2], 3) %>%
  rownames_to_column(var="Term") %>%
  filter(Term != "(Intercept)") %>%
  dplyr::select(-X3)

# R2 and adjusted R2
R2_df <- data.frame(Term = c("R2", "Adjusted R2"),
                    Model3 = c(summary(model3)$r.squared, summary(model3)$adj.r.squared),
                    Model4 = c(summary(model3)$r.squared, summary(model3)$adj.r.squared),
                    Model5 = c(summary(model5)$r.squared, summary(model5)$adj.r.squared))

data.frame(model3_coefs = round(model3$coefficients, 3),
           model3_SE = round(summary(model3)$coefficients[,2], 3),
           model3_pvals = summary(model3)$coefficients[,4],
           model4_coefs = round(model4[,1], 3),
           model4_SE = round(model4[,2], 3),
           model4_pvals = model4[,4]) %>%
  rownames_to_column(var="Term") %>%
  filter(Term != "(Intercept)") %>%
  left_join(., model5_df) %>%
  arrange(model3_pvals) %>%
  mutate(model3_Stars = case_when(model3_pvals < 0.0001 ~ "***",
                                  model3_pvals < 0.001 ~ "**",
                                  model3_pvals < 0.05 ~ "*",
                                  T ~ ""),
         model4_Stars = case_when(model4_pvals < 0.0001 ~ "***",
                                  model4_pvals < 0.001 ~ "**",
                                  model4_pvals < 0.05 ~ "*",
                                  T ~ ""),
         model5_Stars = case_when(model5_pvals < 0.0001 ~ "***",
                                  model5_pvals < 0.001 ~ "**",
                                  model5_pvals < 0.05 ~ "*",
                                  T ~ "")) %>%
  dplyr::select(-model3_pvals, -model4_pvals, -model5_pvals) %>%
  mutate("Model3" = paste0(model3_coefs, model3_Stars, " (", model3_SE, ")"),
         "Model4" = paste0(model4_coefs, model4_Stars, " (", model4_SE, ")"),
         "Model5" = ifelse(is.na(model5_coefs), "", (paste0(model5_coefs, model5_Stars, " (", model5_SE, ")"))),
         .keep = "unused") %>%
  plyr::rbind.fill(., R2_df) %>%
  kable(.) %>%
  kable_styling(full_width = F)
```





# 5. Discussion

# 6. Conclusion

\pagebreak
# 7. References